[
  {
    "id": "hard_1",
    "question": "Why does increasing model capacity not necessarily improve generalization, even with more data?\nA) Because gradient descent becomes unstable\nB) Due to the bias-variance tradeoff and optimization dynamics\nC) Because larger models cannot represent simple functions\nD) Because overfitting only depends on dataset size",
    "correct_answer": "B",
    "correct_option_text": "Due to the bias-variance tradeoff and optimization dynamics"
  },
  {
    "id": "hard_2",
    "question": "Which statement about attention mechanisms is most accurate?\nA) Attention eliminates the need for positional encodings\nB) Attention guarantees interpretability of model decisions\nC) Self-attention allows token interactions independent of distance\nD) Attention reduces model expressiveness",
    "correct_answer": "C",
    "correct_option_text": "Self-attention allows token interactions independent of distance"
  },
  {
    "id": "hard_3",
    "question": "In reinforcement learning, why can off-policy learning be unstable with function approximation?\nA) Because rewards are sparse\nB) Because the Bellman target depends on a changing policy distribution\nC) Because exploration is insufficient\nD) Because episodes are too long",
    "correct_answer": "B",
    "correct_option_text": "Because the Bellman target depends on a changing policy distribution"
  },
  {
    "id": "hard_4",
    "question": "What is the fundamental reason why exact inference in general Bayesian networks is NP-hard?\nA) Continuous probability distributions\nB) Presence of hidden variables\nC) Exponential growth of conditional dependencies\nD) Cyclic graph structures",
    "correct_answer": "C",
    "correct_option_text": "Exponential growth of conditional dependencies"
  },
  {
    "id": "hard_5",
    "question": "Why does training language models with next-token prediction implicitly induce world knowledge?\nA) Because the model memorizes facts directly\nB) Because language structure encodes statistical regularities of the world\nC) Because transformers contain symbolic reasoning modules\nD) Because supervised labels provide semantic grounding",
    "correct_answer": "B",
    "correct_option_text": "Because language structure encodes statistical regularities of the world"
  }
]
